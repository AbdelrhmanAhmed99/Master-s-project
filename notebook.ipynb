{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a006d419",
   "metadata": {},
   "source": [
    "# Seq2Seq Machine Translation (PyTorch)\n",
    "\n",
    "This notebook implements a **Sequence-to-Sequence model with Attention** for machine translation using **PyTorch**.\n",
    "\n",
    "**Architecture:**\n",
    "- **Encoder**: Bidirectional LSTM\n",
    "- **Decoder**: LSTM with Bahdanau-style attention\n",
    "- **Dataset**: WMT14 (source/target vocab pre-tokenized as integer IDs)\n",
    "\n",
    "**Original code** was written in MindSpore — this version is fully re-implemented in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Imports & Device Setup\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import pickle\n",
    "import codecs\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268d455",
   "metadata": {},
   "source": [
    "## 1. Hyperparameters & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f2934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Hyperparameters\n",
    "# ============================================================\n",
    "# ----- Paths -----\n",
    "DATA_PATH = \"./WMT14/\"            # Root folder for dataset\n",
    "SAVE_DIR  = \"./model_WMT14/\"      # Where to save checkpoints\n",
    "\n",
    "# ----- Model -----\n",
    "EMB_DIM   = 128\n",
    "N_HIDDEN  = 256\n",
    "N_LAYER   = 1\n",
    "DROPOUT   = 0.0\n",
    "\n",
    "# ----- Training -----\n",
    "BATCH_SIZE  = 32\n",
    "EPOCHS      = 10\n",
    "LR          = 1e-3\n",
    "CLIP        = 5.0\n",
    "PRINT_FREQ  = 100\n",
    "CKPT_FREQ   = 200\n",
    "PATIENCE    = 5\n",
    "USE_AMP     = True   # Mixed-precision training (set False if no GPU)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999c27c",
   "metadata": {},
   "source": [
    "## 2. Load Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bbf668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Load Vocabularies\n",
    "# ============================================================\n",
    "with open(os.path.join(DATA_PATH, 'raw', 'src_vocab.pkl'), \"rb\") as f:\n",
    "    src_vocab = pickle.load(f)\n",
    "with open(os.path.join(DATA_PATH, 'raw', 'tgt_vocab.pkl'), \"rb\") as f:\n",
    "    tgt_vocab = pickle.load(f)\n",
    "\n",
    "# Ensure <pad> is index 0\n",
    "src_vocab['<pad>'] = 0\n",
    "tgt_vocab['<pad>'] = 0\n",
    "\n",
    "# Build reverse target vocab for decoding\n",
    "tgt_idx2word = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "print(f\"Source vocab size: {len(src_vocab)}\")\n",
    "print(f\"Target vocab size: {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3b: Load Vocabularies from TEXT files  (alternative)\n",
    "# ============================================================\n",
    "# Use this instead of Cell 3 if your vocab files are plain text\n",
    "# with one word per line (not .pkl).\n",
    "\n",
    "def load_vocab(path):\n",
    "    \"\"\"Load a vocab text file (one word per line) → dict[str, int].\"\"\"\n",
    "    word2id = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()\n",
    "            if word and word not in word2id:\n",
    "                word2id[word] = len(word2id)\n",
    "    # Ensure special tokens exist\n",
    "    for tok in ['<pad>', '<unk>', '<s>', '</s>']:\n",
    "        if tok not in word2id:\n",
    "            word2id[tok] = len(word2id)\n",
    "    # Force <pad> = 0 (swap if needed)\n",
    "    if word2id['<pad>'] != 0:\n",
    "        other_word = [k for k, v in word2id.items() if v == 0][0]\n",
    "        old_pad_id = word2id['<pad>']\n",
    "        word2id[other_word] = old_pad_id\n",
    "        word2id['<pad>'] = 0\n",
    "    return word2id\n",
    "\n",
    "src_vocab = load_vocab(os.path.join(DATA_PATH, 'raw', 'src_vocab.txt'))\n",
    "tgt_vocab = load_vocab(os.path.join(DATA_PATH, 'raw', 'tgt_vocab.txt'))\n",
    "\n",
    "# Build reverse target vocab for decoding\n",
    "tgt_idx2word = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "print(f\"Source vocab size: {len(src_vocab)}\")\n",
    "print(f\"Target vocab size: {len(tgt_vocab)}\")\n",
    "print(f\"Sample src entries: {dict(list(src_vocab.items())[:5])}\")\n",
    "print(f\"Sample tgt entries: {dict(list(tgt_vocab.items())[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e812c15",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ba933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Dataset class  (replaces data.py)\n",
    "# ============================================================\n",
    "\n",
    "def tensorize(sents_batch, word2id, device=device):\n",
    "    \"\"\"Pad a list of integer-id sentences to the same length and return a LongTensor.\"\"\"\n",
    "    batch_size = len(sents_batch)\n",
    "    max_len = max(len(s) for s in sents_batch)\n",
    "    PAD = word2id['<pad>']\n",
    "    batch = torch.full((batch_size, max_len), PAD, dtype=torch.long, device=device)\n",
    "    for i, sent in enumerate(sents_batch):\n",
    "        for j, tok in enumerate(sent):\n",
    "            batch[i, j] = tok\n",
    "    return batch\n",
    "\n",
    "\n",
    "def prepro_batch(src_word, tgt_word, batch, device=device):\n",
    "    \"\"\"Prepare a raw (sources, targets) batch for the model.\"\"\"\n",
    "    sources, abstracts = batch\n",
    "    inp_lengths = torch.tensor([len(s) for s in sources], dtype=torch.long, device=device)\n",
    "\n",
    "    # Prepend <s> for decoder input, append </s> for decoder target\n",
    "    tgt_in  = [[tgt_word[\"<s>\"]] + t for t in abstracts]\n",
    "    tgt_out = [t + [tgt_word[\"</s>\"]] for t in abstracts]\n",
    "\n",
    "    sources = tensorize(sources, src_word, device)\n",
    "    tgt_in  = tensorize(tgt_in,  tgt_word, device)\n",
    "    tgt_out = tensorize(tgt_out, tgt_word, device)\n",
    "\n",
    "    return (sources, inp_lengths, tgt_in), tgt_out\n",
    "\n",
    "\n",
    "class GigaDataset:\n",
    "    \"\"\"Simple batch-iterator dataset (mirrors the original MindSpore version).\"\"\"\n",
    "\n",
    "    def __init__(self, path, split, batch_size, src_word, tgt_word):\n",
    "        self.batch_size = batch_size\n",
    "        self.src_word = src_word\n",
    "        self.tgt_word = tgt_word\n",
    "        assert split in ('train', 'val', 'test')\n",
    "\n",
    "        src_file = f\"src_{split}.txt\"\n",
    "        tgt_file = f\"tgt_{split}.txt\"\n",
    "\n",
    "        with codecs.open(os.path.join(path, 'raw', src_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            source_data = f.readlines()\n",
    "        with codecs.open(os.path.join(path, 'raw', tgt_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            target_data = f.readlines()\n",
    "\n",
    "        source_data = [line.rstrip('\\r\\n').split(' ') for line in source_data]\n",
    "        target_data = [line.rstrip('\\r\\n').split(' ') for line in target_data]\n",
    "\n",
    "        # Sort training data by source length for efficient batching\n",
    "        if split == 'train':\n",
    "            data = sorted(zip(source_data, target_data), key=lambda x: len(x[0]))\n",
    "            source_data = [x[0] for x in data]\n",
    "            target_data = [x[1] for x in data]\n",
    "\n",
    "        self.src = source_data\n",
    "        self.tgt = target_data\n",
    "        assert len(self.src) == len(self.tgt)\n",
    "\n",
    "        self.cur_ind = 0\n",
    "        self.tot_batch = len(self.src) // self.batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.cur_ind + self.batch_size >= len(self.src):\n",
    "            self.cur_ind = 0\n",
    "\n",
    "        upper = min(self.cur_ind + self.batch_size, len(self.src))\n",
    "        src = [[int(x) for x in self.src[i]] for i in range(self.cur_ind, upper)]\n",
    "        tgt = [[int(x) for x in self.tgt[i]] for i in range(self.cur_ind, upper)]\n",
    "\n",
    "        self.cur_ind = upper if upper < len(self.src) else 0\n",
    "\n",
    "        return prepro_batch(self.src_word, self.tgt_word, [src, tgt])\n",
    "\n",
    "print(\"Dataset class ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec06d2",
   "metadata": {},
   "source": [
    "## 4. Model Definition — Seq2Seq with Attention (PyTorch)\n",
    "\n",
    "The model consists of:\n",
    "- **Encoder**: Bidirectional LSTM that reads the source sentence\n",
    "- **Attention Decoder**: LSTM decoder with Bahdanau-style additive attention over encoder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Attention Decoder  (replaces AttnDecoder in Seq2Seq.py)\n",
    "# ============================================================\n",
    "\n",
    "class AttnDecoder(nn.Module):\n",
    "    \"\"\"LSTM decoder with Bahdanau-style attention.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding, hidden_size, output_size,\n",
    "                 enc_out_dim, n_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.n_layers = n_layers\n",
    "        emb_size = embedding.embedding_dim\n",
    "\n",
    "        self.decoder_cell = nn.LSTMCell(emb_size, hidden_size)\n",
    "        self.attn = nn.Linear(enc_out_dim, hidden_size, bias=False)\n",
    "        self.concat = nn.Linear(enc_out_dim + hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, target, init_states, enc_outs, attn_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target:      (batch, tgt_len)  — decoder input token ids\n",
    "            init_states: tuple (h0, c0) each (batch, hidden)\n",
    "            enc_outs:    (src_len, batch, enc_out_dim)\n",
    "            attn_mask:   (batch, src_len)  — 1 = valid, 0 = pad\n",
    "        Returns:\n",
    "            logits: (batch, tgt_len, vocab)\n",
    "        \"\"\"\n",
    "        max_len = target.size(1)\n",
    "        h, c = init_states\n",
    "        logits = []\n",
    "        for i in range(max_len):\n",
    "            inp_i = target[:, i:i+1]                       # (batch, 1)\n",
    "            logit, (h, c) = self._step(inp_i, (h, c), enc_outs, attn_mask)\n",
    "            logits.append(logit)\n",
    "        return torch.stack(logits, dim=1)                   # (batch, tgt_len, vocab)\n",
    "\n",
    "    def _step(self, inp, last_hidden, enc_outs, attn_mask):\n",
    "        \"\"\"Single decoding step.\"\"\"\n",
    "        h_prev, c_prev = last_hidden\n",
    "        embed = self.embedding(inp).squeeze(1)              # (batch, emb)\n",
    "        h_t, c_t = self.decoder_cell(embed, (h_prev, c_prev))\n",
    "\n",
    "        # --- attention ---\n",
    "        attn_scores = self._get_attn(h_t, enc_outs, attn_mask)  # (batch, 1, src_len)\n",
    "        context = attn_scores.bmm(enc_outs.transpose(0, 1))     # (batch, 1, enc_dim)\n",
    "        context = context.squeeze(1)                             # (batch, enc_dim)\n",
    "\n",
    "        concat_out = torch.tanh(self.concat(torch.cat([context, h_t], dim=1)))\n",
    "        logit = F.log_softmax(self.out(concat_out), dim=-1)      # (batch, vocab)\n",
    "        return logit, (h_t, c_t)\n",
    "\n",
    "    def _get_attn(self, dec_out, enc_outs, attn_mask):\n",
    "        \"\"\"Compute attention weights.\"\"\"\n",
    "        # enc_outs: (src_len, batch, enc_dim)\n",
    "        query = dec_out.unsqueeze(0)                             # (1, batch, hidden)\n",
    "        keys = self.attn(enc_outs)                               # (src_len, batch, hidden)\n",
    "        weights = (query * keys).sum(dim=2)                      # (src_len, batch)\n",
    "        weights = weights.transpose(0, 1)                        # (batch, src_len)\n",
    "        weights = weights.masked_fill(attn_mask == 0, -1e18)\n",
    "        weights = F.softmax(weights, dim=1).unsqueeze(1)         # (batch, 1, src_len)\n",
    "        return weights\n",
    "\n",
    "print(\"AttnDecoder ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Seq2SeqSum  (replaces Seq2Seq.py main class)\n",
    "# ============================================================\n",
    "\n",
    "def len_mask(lens, device=device):\n",
    "    \"\"\"Create a (batch, max_len) boolean mask from a list/tensor of lengths.\"\"\"\n",
    "    max_len = int(max(lens))\n",
    "    batch_size = len(lens)\n",
    "    mask = torch.zeros(batch_size, max_len, dtype=torch.bool, device=device)\n",
    "    for i, l in enumerate(lens):\n",
    "        mask[i, :int(l)] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "class Seq2SeqSum(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim,\n",
    "                 n_hidden, n_layer=1, bi_enc=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n_layer = n_layer\n",
    "        self.bi_enc = bi_enc\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        # Shared embedding (used by both encoder & decoder)\n",
    "        self.embedding = nn.Embedding(src_vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            emb_dim, n_hidden, n_layer,\n",
    "            bidirectional=bi_enc,\n",
    "            dropout=0.0 if n_layer == 1 else dropout,\n",
    "        )\n",
    "        num_dirs = 2 if bi_enc else 1\n",
    "        self.enc_out_dim = n_hidden * num_dirs\n",
    "\n",
    "        # Learnable initial encoder states\n",
    "        self.enc_init_h = nn.Parameter(torch.empty(n_layer * num_dirs, n_hidden).uniform_(-1e-2, 1e-2))\n",
    "        self.enc_init_c = nn.Parameter(torch.empty(n_layer * num_dirs, n_hidden).uniform_(-1e-2, 1e-2))\n",
    "\n",
    "        # Project encoder final hidden → decoder initial hidden\n",
    "        self._dec_h = nn.Linear(self.enc_out_dim, n_hidden, bias=False)\n",
    "        self._dec_c = nn.Linear(self.enc_out_dim, n_hidden, bias=False)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = AttnDecoder(\n",
    "            self.embedding, n_hidden, tgt_vocab_size,\n",
    "            self.enc_out_dim, n_layer, dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_lengths, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src:         (batch, src_len) — source token ids\n",
    "            src_lengths: (batch,)         — true lengths (before padding)\n",
    "            tgt:         (batch, tgt_len) — decoder input token ids (with <s>)\n",
    "        Returns:\n",
    "            logits: (batch, tgt_len, tgt_vocab)\n",
    "        \"\"\"\n",
    "        enc_outs, init_dec_states = self.encode(src, src_lengths)\n",
    "        attn_mask = len_mask(src_lengths, src.device)\n",
    "        logits = self.decoder(tgt, init_dec_states, enc_outs, attn_mask)\n",
    "        return logits\n",
    "\n",
    "    def encode(self, src, src_lengths):\n",
    "        \"\"\"Encode source sequence and return encoder outputs + initial decoder state.\"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        # Expand learnable init states → (layers*dirs, batch, hidden)\n",
    "        h0 = self.enc_init_h.unsqueeze(1).expand(-1, batch_size, -1).contiguous()\n",
    "        c0 = self.enc_init_c.unsqueeze(1).expand(-1, batch_size, -1).contiguous()\n",
    "\n",
    "        embed = self.embedding(src).transpose(0, 1)  # (src_len, batch, emb)\n",
    "        enc_out, (h, c) = self.encoder(embed, (h0, c0))  # enc_out: (src_len, batch, enc_dim)\n",
    "\n",
    "        if self.bi_enc:\n",
    "            # Merge bidirectional hidden states: (2*layers, batch, hidden) → (layers, batch, 2*hidden)\n",
    "            h = torch.cat(h.chunk(2, dim=0), dim=2)\n",
    "            c = torch.cat(c.chunk(2, dim=0), dim=2)\n",
    "\n",
    "        # Project to decoder hidden size\n",
    "        dec_h = self._dec_h(h).squeeze(0)  # (batch, hidden)\n",
    "        dec_c = self._dec_c(c).squeeze(0)\n",
    "        return enc_out, (dec_h, dec_c)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Beam search (used at inference time)\n",
    "    # ------------------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def beam_decode(self, inp, src_vocab, tgt_vocab, beam_size=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp:  list of integer token ids for ONE source sentence\n",
    "            beam_size: number of beams\n",
    "        Returns:\n",
    "            List of (token_id_list, score) sorted by score descending.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        inp_t = torch.tensor([inp], dtype=torch.long, device=device)\n",
    "        inp_len = torch.tensor([len(inp)], dtype=torch.long, device=device)\n",
    "        attn_mask = torch.ones_like(inp_t, dtype=torch.bool, device=device)\n",
    "\n",
    "        SOS = tgt_vocab[\"<s>\"]\n",
    "        EOS = tgt_vocab[\"</s>\"]\n",
    "        tgt_vocab_size = len(tgt_vocab)\n",
    "        k = 50  # max completed hypotheses\n",
    "\n",
    "        enc_outs, (h, c) = self.encode(inp_t, inp_len)\n",
    "        # Expand for beam\n",
    "        h = h.expand(beam_size, -1).contiguous()       # (beam, hidden)\n",
    "        c = c.expand(beam_size, -1).contiguous()\n",
    "\n",
    "        top_k_scores = torch.zeros(beam_size, device=device)\n",
    "        top_k_words = torch.full((beam_size, 1), SOS, dtype=torch.long, device=device)\n",
    "        prev_words = top_k_words\n",
    "\n",
    "        completed_seqs = []\n",
    "        completed_scores = []\n",
    "\n",
    "        for step in range(1, 33):  # max 32 decoding steps\n",
    "            logit, (h, c) = self.decoder._step(\n",
    "                prev_words, (h, c), enc_outs, attn_mask\n",
    "            )\n",
    "            log_probs = F.log_softmax(logit, dim=1)  # (beam, vocab)\n",
    "            log_probs = top_k_scores.unsqueeze(1) + log_probs\n",
    "\n",
    "            if step == 1:\n",
    "                cur_beam = min(log_probs.size(1), beam_size)\n",
    "                top_k_scores, top_k_ids = log_probs[0].topk(cur_beam)\n",
    "            else:\n",
    "                cur_beam = min(log_probs.view(-1).size(0), beam_size)\n",
    "                top_k_scores, top_k_ids = log_probs.view(-1).topk(cur_beam)\n",
    "\n",
    "            beam_idx = top_k_ids // tgt_vocab_size\n",
    "            word_idx = top_k_ids % tgt_vocab_size\n",
    "\n",
    "            top_k_words = torch.cat([top_k_words[beam_idx], word_idx.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # Separate complete / incomplete\n",
    "            incomplete = [i for i, w in enumerate(word_idx) if w.item() != EOS]\n",
    "            complete   = [i for i in range(len(word_idx)) if i not in incomplete]\n",
    "\n",
    "            if complete:\n",
    "                completed_seqs.extend(top_k_words[complete].tolist())\n",
    "                completed_scores.extend(top_k_scores[complete].tolist())\n",
    "                k -= len(complete)\n",
    "\n",
    "            if k <= 0:\n",
    "                break\n",
    "\n",
    "            # Keep only incomplete beams\n",
    "            top_k_words  = top_k_words[incomplete]\n",
    "            top_k_scores = top_k_scores[incomplete]\n",
    "            h = h[beam_idx[incomplete]]\n",
    "            c = c[beam_idx[incomplete]]\n",
    "            prev_words = top_k_words[:, -1:]\n",
    "\n",
    "            if top_k_words.size(0) == 0:\n",
    "                break\n",
    "\n",
    "        # Sort by score\n",
    "        results = sorted(zip(completed_seqs, completed_scores),\n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "        return results\n",
    "\n",
    "print(\"Seq2SeqSum model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc6948",
   "metadata": {},
   "source": [
    "## 5. Trainer  (replaces training.py)\n",
    "\n",
    "Handles the training loop, validation, gradient clipping, checkpointing, and early stopping — all using PyTorch idioms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95281ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Trainer class  (replaces training.py)\n",
    "# ============================================================\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader,\n",
    "                 save_dir, clip, print_freq, ckpt_freq, patience, epochs,\n",
    "                 use_amp=True):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.save_dir = save_dir\n",
    "        self.clip = clip\n",
    "        self.print_freq = print_freq\n",
    "        self.ckpt_freq = ckpt_freq\n",
    "        self.patience = patience\n",
    "        self.epochs = epochs\n",
    "        self.use_amp = use_amp and torch.cuda.is_available()\n",
    "\n",
    "        self.scaler = GradScaler(enabled=self.use_amp)\n",
    "        self.step = 0\n",
    "        self.cur_epoch = 1\n",
    "        self.current_p = 0\n",
    "        self.best_val = float('inf')\n",
    "\n",
    "    # ---- loss -------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def compute_loss(logits, targets, pad_idx=0):\n",
    "        \"\"\"NLL loss ignoring <pad> positions.\"\"\"\n",
    "        # logits: (batch, tgt_len, vocab)   targets: (batch, tgt_len)\n",
    "        mask = (targets != pad_idx).view(-1)\n",
    "        logits_flat = logits.view(-1, logits.size(2))[mask]\n",
    "        targets_flat = targets.view(-1)[mask]\n",
    "        return F.nll_loss(logits_flat, targets_flat)\n",
    "\n",
    "    # ---- single training step ---------------------------------------\n",
    "    def train_step(self, srcs, targets):\n",
    "        src, src_lens, tgt = srcs\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        with autocast(enabled=self.use_amp):\n",
    "            logits = self.model(src, src_lens, tgt)\n",
    "            loss = self.compute_loss(logits, targets)\n",
    "\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        self.step += 1\n",
    "        return loss.item()\n",
    "\n",
    "    # ---- validation -------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        val_batches = min(100, self.val_loader.tot_batch)\n",
    "        for _ in range(val_batches):\n",
    "            srcs, targets = self.val_loader.next_batch()\n",
    "            src, src_lens, tgt = srcs\n",
    "            with autocast(enabled=self.use_amp):\n",
    "                logits = self.model(src, src_lens, tgt)\n",
    "                loss = self.compute_loss(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "        avg = total_loss / val_batches\n",
    "        print(f\"  Epoch {self.cur_epoch}  |  Val loss: {avg:.4f}\")\n",
    "        return avg\n",
    "\n",
    "    # ---- checkpoint -------------------------------------------------\n",
    "    def checkpoint(self):\n",
    "        name = f\"ckpt-{self.cur_epoch}e-{self.step}s.pt\"\n",
    "        path = os.path.join(self.save_dir, name)\n",
    "        torch.save({\n",
    "            'epoch': self.cur_epoch,\n",
    "            'step': self.step,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'best_val': self.best_val,\n",
    "        }, path)\n",
    "        print(f\"  Checkpoint saved → {path}\")\n",
    "\n",
    "    # ---- early stopping check ---------------------------------------\n",
    "    def check_stop(self, val_loss):\n",
    "        if val_loss < self.best_val:\n",
    "            self.best_val = val_loss\n",
    "            self.checkpoint()\n",
    "            self.current_p = 0\n",
    "        else:\n",
    "            self.current_p += 1\n",
    "        return self.current_p >= self.patience\n",
    "\n",
    "    # ---- logging ----------------------------------------------------\n",
    "    def log_info(self, running_loss):\n",
    "        total_steps = self.train_loader.tot_batch\n",
    "        pct = 100 * self.step / total_steps\n",
    "        avg = running_loss / self.print_freq\n",
    "        print(f\"  Epoch {self.cur_epoch} | step {self.step}/{total_steps} \"\n",
    "              f\"({pct:.1f}%) | loss {avg:.4f}\")\n",
    "\n",
    "    # ---- main training loop ----------------------------------------\n",
    "    def train(self):\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.cur_epoch = epoch\n",
    "            self.model.train()\n",
    "            self.step = 0\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for _ in range(self.train_loader.tot_batch):\n",
    "                srcs, targets = self.train_loader.next_batch()\n",
    "                step_loss = self.train_step(srcs, targets)\n",
    "                running_loss += step_loss\n",
    "\n",
    "                if self.step % self.print_freq == 0:\n",
    "                    self.log_info(running_loss)\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            val_loss = self.validate()\n",
    "            self.checkpoint()\n",
    "\n",
    "            if self.check_stop(val_loss):\n",
    "                print(\"Early stopping — finished training!\")\n",
    "                return\n",
    "\n",
    "        print(\"Reached max epochs — finished training!\")\n",
    "\n",
    "print(\"Trainer ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debe7b9",
   "metadata": {},
   "source": [
    "## 6. Instantiate Model, Data & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Build everything\n",
    "# ============================================================\n",
    "\n",
    "# Data loaders\n",
    "train_loader = GigaDataset(DATA_PATH, 'train', BATCH_SIZE, src_vocab, tgt_vocab)\n",
    "val_loader   = GigaDataset(DATA_PATH, 'val',   BATCH_SIZE, src_vocab, tgt_vocab)\n",
    "\n",
    "print(f\"Training batches : {train_loader.tot_batch}\")\n",
    "print(f\"Validation batches: {val_loader.tot_batch}\")\n",
    "\n",
    "# Model\n",
    "model = Seq2SeqSum(\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    emb_dim=EMB_DIM,\n",
    "    n_hidden=N_HIDDEN,\n",
    "    n_layer=N_LAYER,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters : {total_params:,}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61c625",
   "metadata": {},
   "source": [
    "## 7. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Run training\n",
    "# ============================================================\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    save_dir=SAVE_DIR,\n",
    "    clip=CLIP,\n",
    "    print_freq=PRINT_FREQ,\n",
    "    ckpt_freq=CKPT_FREQ,\n",
    "    patience=PATIENCE,\n",
    "    epochs=EPOCHS,\n",
    "    use_amp=USE_AMP,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda2051",
   "metadata": {},
   "source": [
    "## 8. Beam Search Decoding  (replaces decode.py)\n",
    "\n",
    "Load a trained checkpoint and run beam-search on the test set, writing predictions to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888284f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Beam-search decoding  (replaces decode.py)\n",
    "# ============================================================\n",
    "\n",
    "# ---- Settings (adjust after training) ----\n",
    "MODEL_CKPT  = os.path.join(SAVE_DIR, \"ckpt-6e-0s.pt\")   # path to best checkpoint\n",
    "BEAM_SIZE   = 50\n",
    "OUTPUT_DIR  = \"./output/\"\n",
    "OUT_FILE    = os.path.join(OUTPUT_DIR, \"WMT14_output.txt\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_model(ckpt_path, src_vocab, tgt_vocab):\n",
    "    \"\"\"Load a Seq2SeqSum model from a PyTorch checkpoint.\"\"\"\n",
    "    model = Seq2SeqSum(\n",
    "        len(src_vocab), len(tgt_vocab),\n",
    "        EMB_DIM, N_HIDDEN, N_LAYER\n",
    "    ).to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_beam_search(model, test_src, test_tgt, tgt_idx2word,\n",
    "                    tgt_vocab, fout, beam_size=50):\n",
    "    \"\"\"Run beam search over the test set, write predictions to file.\"\"\"\n",
    "    for idx, (src_ids, tgt_ids) in enumerate(zip(test_src, test_tgt)):\n",
    "        results = model.beam_decode(src_ids, src_vocab, tgt_vocab, beam_size)\n",
    "\n",
    "        if len(results) < 1:\n",
    "            pred = []\n",
    "        else:\n",
    "            pred = [tgt_idx2word.get(x, '<unk>') for x in results[0][0]]\n",
    "            pred = pred[1:-1]  # strip <s> and </s>\n",
    "\n",
    "        tgt_str = [tgt_idx2word.get(x, '<unk>') for x in tgt_ids]\n",
    "        fout.write(f\"{pred}\\n{tgt_str}\\n\")\n",
    "\n",
    "        if (idx + 1) % 50 == 0 or idx == 0:\n",
    "            print(f\"[{idx+1}] pred: {pred}\")\n",
    "            print(f\"     tgt : {tgt_str}\\n\")\n",
    "\n",
    "\n",
    "# ---- Load test data ----\n",
    "def load_test_file(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\r\\n')\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append([int(x) for x in line.split(' ')])\n",
    "    return lines\n",
    "\n",
    "\n",
    "# ---- Uncomment below to run decoding after training ----\n",
    "# test_src = load_test_file(os.path.join(DATA_PATH, 'raw', 'src_test.txt'))\n",
    "# test_tgt = load_test_file(os.path.join(DATA_PATH, 'raw', 'tgt_test.txt'))\n",
    "# decode_model = load_model(MODEL_CKPT, src_vocab, tgt_vocab)\n",
    "# with open(OUT_FILE, 'w', encoding='ISO-8859-1') as fout:\n",
    "#     run_beam_search(decode_model, test_src, test_tgt, tgt_idx2word, tgt_vocab, fout, BEAM_SIZE)\n",
    "# print(f\"Decoding complete → {OUT_FILE}\")\n",
    "\n",
    "print(\"Decoding functions ready. Uncomment the block above after training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1ffda",
   "metadata": {},
   "source": [
    "## 9. Evaluation — BLEU & ROUGE  (replaces eval.py)\n",
    "\n",
    "Compute BLEU-1/2/3/4 and ROUGE-1/2/L scores from the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515714af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Evaluation  (replaces eval.py)\n",
    "# ============================================================\n",
    "# Requires: pip install nltk rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "def bleu_scores(pred, tgt):\n",
    "    \"\"\"Compute BLEU 1-4 for a single sentence pair.\"\"\"\n",
    "    pred = [str(x) for x in pred]\n",
    "    b1 = sentence_bleu([pred], tgt, weights=(1, 0, 0, 0))\n",
    "    b2 = sentence_bleu([pred], tgt, weights=(0, 1, 0, 0))\n",
    "    b3 = sentence_bleu([pred], tgt, weights=(0, 0, 1, 0))\n",
    "    b4 = sentence_bleu([pred], tgt, weights=(0, 0, 0, 1))\n",
    "    return b1, b2, b3, b4\n",
    "\n",
    "\n",
    "def rouge_scores(pred, tgt):\n",
    "    \"\"\"Compute ROUGE for a single sentence pair.\"\"\"\n",
    "    rouge = Rouge()\n",
    "    hyp = ' '.join(str(x) for x in pred)\n",
    "    ref = ' '.join(tgt)\n",
    "    return rouge.get_scores(hyps=hyp, refs=ref)[0]\n",
    "\n",
    "\n",
    "def evaluate(output_path, result_path):\n",
    "    \"\"\"Read the decode output file, compute aggregate metrics, write results.\"\"\"\n",
    "    with open(output_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    tot = 0\n",
    "    bleu_agg = [0.0, 0.0, 0.0, 0.0]\n",
    "    rouge_1, rouge_2, rouge_l = defaultdict(float), defaultdict(float), defaultdict(float)\n",
    "\n",
    "    idx = 0\n",
    "    while idx < len(data):\n",
    "        pred = eval(data[idx].rstrip('\\r\\n'));  idx += 1\n",
    "        tgt  = eval(data[idx].rstrip('\\r\\n'));  idx += 1\n",
    "\n",
    "        if not pred:\n",
    "            b = (0, 0, 0, 0)\n",
    "            r = {'rouge-1': {'f': 0, 'p': 0, 'r': 0},\n",
    "                 'rouge-2': {'f': 0, 'p': 0, 'r': 0},\n",
    "                 'rouge-l': {'f': 0, 'p': 0, 'r': 0}}\n",
    "        else:\n",
    "            b = bleu_scores(pred, tgt)\n",
    "            r = rouge_scores(pred, tgt)\n",
    "\n",
    "        for i in range(4):\n",
    "            bleu_agg[i] += b[i]\n",
    "        for k in ('f', 'p', 'r'):\n",
    "            rouge_1[k] += r['rouge-1'][k]\n",
    "            rouge_2[k] += r['rouge-2'][k]\n",
    "            rouge_l[k] += r['rouge-l'][k]\n",
    "        tot += 1\n",
    "\n",
    "    # Print & save\n",
    "    lines = []\n",
    "    lines.append(f\"Total samples: {tot}\")\n",
    "    lines.append(f\"BLEU-1: {bleu_agg[0]/tot:.4f}  BLEU-2: {bleu_agg[1]/tot:.4f}  \"\n",
    "                 f\"BLEU-3: {bleu_agg[2]/tot:.4f}  BLEU-4: {bleu_agg[3]/tot:.4f}\")\n",
    "    lines.append(f\"ROUGE-1  r:{rouge_1['r']/tot:.4f}  p:{rouge_1['p']/tot:.4f}  f:{rouge_1['f']/tot:.4f}\")\n",
    "    lines.append(f\"ROUGE-2  r:{rouge_2['r']/tot:.4f}  p:{rouge_2['p']/tot:.4f}  f:{rouge_2['f']/tot:.4f}\")\n",
    "    lines.append(f\"ROUGE-L  r:{rouge_l['r']/tot:.4f}  p:{rouge_l['p']/tot:.4f}  f:{rouge_l['f']/tot:.4f}\")\n",
    "\n",
    "    for l in lines:\n",
    "        print(l)\n",
    "\n",
    "    with open(result_path, 'w') as fout:\n",
    "        fout.write('\\n'.join(lines))\n",
    "    print(f\"\\nResults saved → {result_path}\")\n",
    "\n",
    "\n",
    "# ---- Uncomment after decoding ----\n",
    "# evaluate(OUT_FILE, os.path.join(OUTPUT_DIR, \"WMT14_result.txt\"))\n",
    "\n",
    "print(\"Evaluation functions ready. Uncomment the line above after decoding.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
